{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6860154d-5760-4c6e-a55a-f65e41c147cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "import gradio as gr\n",
    "import os\n",
    "import openai\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661cb30f-e2e1-4874-84f6-7d20c4fedb54",
   "metadata": {},
   "source": [
    "## Ê∏¨Ë©¶gpt api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "277e89ba-da6b-4e35-a588-07842518f897",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/texes.txt', 'r', encoding='utf-8') as fh:\n",
    "    tmp = fh.read()\n",
    "    itemlist = tmp.split(',')\n",
    "\n",
    "itemlist = str(itemlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "925001f9-3e5e-4436-81c6-bbbbf6880567",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyfile = open(\"key.txt\", \"r\")\n",
    "key = keyfile.readline()\n",
    "\n",
    "openai.api_key = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae79f28-3a0a-46be-b33d-6a21191b6610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_idx = 0\n",
    "# result = ''\n",
    "# while start_idx < len(itemlist):\n",
    "#     end_idx = min(start_idx + 1600, len(itemlist))\n",
    "#     sub_list = itemlist[start_idx:end_idx]\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=\"ada\",\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a chatbot\"},\n",
    "#             {\"role\": \"user\", \"content\": f\"Êèê‰æõ‰ª•‰∏äÊñáÂ≠ó‰πãÁπÅÈ´î‰∏≠ÊñáÊëòË¶ÅËàáËã±ÊñáÁøªË≠ØÔºö{sub_list}\"}\n",
    "#         ]\n",
    "#     )  \n",
    "#     for choice in response.choices:\n",
    "#         result += choice.message.content\n",
    "#     print(start_idx)\n",
    "#     start_idx = end_idx\n",
    "\n",
    "# with open('output.txt', 'w', encoding='utf-8') as output_file:\n",
    "#     output_file.write(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06bcf9d-dcd7-430f-ae33-86e2682a7b91",
   "metadata": {},
   "source": [
    "## ËôïÁêÜÂ∑≤Ë®ìÁ∑¥Ê®°ÁµÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e21775-11df-4b77-b78f-f92ad77691e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-02 13:45:41.738417: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model(\"model_epoch_06.h5\", compile=False)\n",
    "model_config = loaded_model.get_config()\n",
    "#loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40fcfd1b-0bbf-41f6-bcb5-73951ca3485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(img):\n",
    "\n",
    "    # Resize the image to match the input shape of your model\n",
    "    img = img.resize((256, 256))  # Adjust the size as needed\n",
    "    # Convert the image to a NumPy array\n",
    "    img_np = np.array(img)\n",
    "    \n",
    "    # Preprocess the image to match the model's input requirements\n",
    "    img_np = keras_image.img_to_array(img_np)\n",
    "    \n",
    "    img_for_plot = img_np / 255.0  # Normalize the image if necessary\n",
    "    \n",
    "    img_np = np.expand_dims(img_np, axis=0)  # Add a batch dimension\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = loaded_model.predict(img_np)\n",
    "    \n",
    "    return img_for_plot, predictions\n",
    "\n",
    "def image_mod(img):\n",
    "    \n",
    "    image, predictions = pred(img)\n",
    "\n",
    "    print(predictions)\n",
    "    \n",
    "    result = \"\"\n",
    "    \n",
    "    if predictions[0][0] < 0.5:\n",
    "\n",
    "        result = \"fake image\"\n",
    "    else :\n",
    "        \n",
    "        result = \"true image\"\n",
    "\n",
    "    return result\n",
    "\n",
    "def chatbtn(content):\n",
    "    \n",
    "    print(content)\n",
    "    \n",
    "    return content\n",
    "\n",
    "def display_image_from_video(video):\n",
    "\n",
    "    capture_image = cv.VideoCapture(video) \n",
    "    # Áç≤ÂèñË¶ñÈ†ª‰∏≠ÁöÑ‰∏ÄÂπÄÔºåËÆäÊï∏ÂàÜÂà•ÁÇ∫ÊòØÂê¶ÊàêÂäüËÆÄÂèñ‰∫Ü‰∏ÄÂπÄ‰ª•ÂèäÊçïÁç≤ÁöÑÂúñÂÉèÂπÄ\n",
    "    ret, frame = capture_image.read()\n",
    "\n",
    "    img = cv.cvtColor(frame, cv.COLOR_BGR2RGB)\n",
    "    img = cv.resize(img, (256, 256))  # Ë™øÊï¥Â§ßÂ∞è\n",
    "    img_for_plot = img / 255.0  # Ê≠£Ë¶èÂåñÂúñÂÉè\n",
    "    \n",
    "    img_np = np.expand_dims(img, axis=0)  # Add a batch dimension\n",
    "    \n",
    "#     # Make predictions\n",
    "#     predictions = loaded_model.predict(img_np)\n",
    "    \n",
    "#     result = \"\"\n",
    "    \n",
    "#     if predictions[0][0] < 0.5:\n",
    "\n",
    "#         result = \"fake image\"\n",
    "#     else :\n",
    "        \n",
    "#         result = \"true image\"\n",
    "\n",
    "#     return result\n",
    "    return img_for_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "205507dd-3ddc-4642-9da9-606598788e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_port = 0\n",
    "cam = cv2.VideoCapture(cam_port) \n",
    "# reading the input using the camera \n",
    "result, image = cam.read() \n",
    "# If image will detected without any error,  \n",
    "# show result \n",
    "if result: \n",
    "    # showing result, it take frame name and image  \n",
    "    # output \n",
    "    cv2.imshow(\"images/GeeksForGeeks\", image) \n",
    "    # saving image in local storage \n",
    "    cv2.imwrite(\"images/GeeksForGeeks.png\", image) \n",
    "    # If keyboard interrupt occurs, destroy image  \n",
    "    # window \n",
    "    # cv2.waitKey(0) \n",
    "    # cv2.destroyWindow(\"GeeksForGeeks\") \n",
    "else: \n",
    "    print(\"No image detected. Please! try again\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d534885-2796-4907-9e69-9ec81d275c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step\n",
      "[[1. 0.]]\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = \"\"\"<h1 align=\"center\">AIËáâÈÉ®Ëæ®Ë≠òüöÄ</h1>\"\"\"\n",
    "                \n",
    "with gr.Blocks(css = \"\"\".gradio-container {background-color: #3f7791}\"\"\") as demo:\n",
    "    gr.HTML(title)\n",
    "    gr.HTML('''<center><a href=\"https://github.com/kennywang112?tab=repositories\" alt=\"GitHub Repo\"></a></center>''')\n",
    "    with gr.Row():\n",
    "        \n",
    "        with gr.Column(elem_id = \"col_container\"):\n",
    "            \n",
    "            chatbot = gr.Chatbot(elem_id='chatbot')\n",
    "            inputs = gr.Textbox(placeholder= \"Hi there!\", label= \"Type an input and press Enter\")\n",
    "            state = gr.State([])\n",
    "            btn = gr.Button('ask') \n",
    "            \n",
    "        btn.click(chatbtn, [inputs], [])\n",
    "\n",
    "        with gr.Column(elem_id = \"col_container\"):\n",
    "            \n",
    "            gr.Interface(\n",
    "                image_mod,\n",
    "                gr.Image(type=\"pil\"),\n",
    "                gr.components.Text(),\n",
    "                examples=[\n",
    "                    os.path.join(\"images/real_face0.jpeg\"),\n",
    "                    os.path.join(\"images/real_face1.jpeg\"),\n",
    "                    os.path.join(\"images/fake_face1.jpeg\"),\n",
    "                    os.path.join(\"images/GeeksForGeeks.png\"),\n",
    "                ],\n",
    "            )\n",
    "                \n",
    "        with gr.Column(elem_id = \"col_container\"):\n",
    "            \n",
    "            inputs = gr.components.Video()\n",
    "            outputs = gr.components.Text()\n",
    "            # outputs = gr.components.Image()\n",
    "            update = gr.Button('Âà§Êñ∑ÂΩ±ÁâáÂúñÂÉè')\n",
    "\n",
    "            update.click(display_image_from_video, inputs = [inputs], outputs = [outputs])\n",
    "    \n",
    "demo.queue().launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
